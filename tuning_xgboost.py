# -*- coding: utf-8 -*-
"""Tuning_XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y_WwltV4xjqrwKmMuzXz35fMZq8UbcYi
"""

!pip install xgboost

# Import required libraries
from google.colab import files

# Upload your kaggle.json file for API access
files.upload()

# Create a directory for the Kaggle API key
!mkdir -p ~/.kaggle

# Move the kaggle.json file into the Kaggle directory
!mv kaggle.json ~/.kaggle/

# Set permissions for the kaggle.json file
!chmod 600 ~/.kaggle/kaggle.json

# Download the dataset using the Kaggle API (use the dataset you specified)
!kaggle datasets download -d sudalairajkumar/daily-temperature-of-major-cities

# Unzip the dataset
!unzip daily-temperature-of-major-cities.zip

import pandas as pd

# Load the dataset
df = pd.read_csv('city_temperature.csv')

# Check the original size of the dataset
print("Original dataset size:", df.shape)

# Sample 10% of the dataset to avoid memory issues
df = df.sample(frac=0.01, random_state=42)

# Check the new size of the dataset
print("Reduced dataset size:", df.shape)

# Inspect the first few rows to check the data
df.head()

# Drop irrelevant columns: 'State', 'Year', and 'Day'
df = df.drop(columns=['State', 'Year', 'Day'])

# Filter out invalid temperature values (e.g., AvgTemperature < -50)
df = df[df['AvgTemperature'] > -50]

# Extract the 'Month' as is (it’s already numeric)
df['Month'] = df['Month']

# Drop rows with missing data (if any remain)
df = df.dropna()

# One-hot encode categorical columns 'Region', 'Country', and 'City'
df = pd.get_dummies(df, columns=['Region', 'Country', 'City'], drop_first=True)

# Check the final dataset's info to ensure all columns are numeric
df.info()

# Check the final shape of the dataset
print("Final dataset shape after encoding:", df.shape)

# Define features (X) and target (y)
X = df.drop(columns=['AvgTemperature'])  # Target is 'AvgTemperature'
y = df['AvgTemperature']

# Check the shapes of X and y
print("X shape:", X.shape)
print("y shape:", y.shape)

from sklearn.model_selection import train_test_split

# Split the dataset into 80% training and 20% testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shapes of the training and testing sets
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# Import XGBoost library
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Create an XGBoost Regressor model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Train the model on the training data
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_test_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model performance
test_mse_xgb = mean_squared_error(y_test, y_test_pred_xgb)
test_r2_xgb = r2_score(y_test, y_test_pred_xgb)

# Print the performance metrics
print('XGBoost Regression Performance:')
print(f'Test MSE: {test_mse_xgb}')
print(f'Test R²: {test_r2_xgb}')

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0]
}

# Set up GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)

# Fit GridSearchCV on the training data
grid_search.fit(X_train, y_train)

# Print the best parameters found by GridSearchCV
print(f'Best Parameters: {grid_search.best_params_}')

# Refit the model with the best parameters
best_xgb_model = grid_search.best_estimator_

# Make predictions on the test set with the tuned model
y_test_pred_best_xgb = best_xgb_model.predict(X_test)

# Evaluate the tuned model performance
test_mse_best_xgb = mean_squared_error(y_test, y_test_pred_best_xgb)
test_r2_best_xgb = r2_score(y_test, y_test_pred_best_xgb)

# Print the performance metrics
print('Tuned XGBoost Regression Performance:')
print(f'Test MSE: {test_mse_best_xgb}')
print(f'Test R²: {test_r2_best_xgb}')

import matplotlib.pyplot as plt

# Create a scatter plot for actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred_best_xgb, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual AvgTemperature')
plt.ylabel('Predicted AvgTemperature')
plt.title('XGBoost: Actual vs. Predicted AvgTemperature')
plt.show()

from sklearn.model_selection import RandomizedSearchCV

# Define a broader parameter grid for tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'subsample': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.3, 0.5],
    'min_child_weight': [1, 3, 5]
}

# Use RandomizedSearchCV to search the parameter grid
randomized_search = RandomizedSearchCV(
    estimator=xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
    param_distributions=param_grid,
    n_iter=20,  # Number of random combinations to try
    cv=3,  # 3-fold cross-validation
    scoring='neg_mean_squared_error',  # Using MSE as the scoring metric
    verbose=1,
    random_state=42
)

# Fit RandomizedSearchCV on the training data
randomized_search.fit(X_train, y_train)

# Output the best parameters
print(f'Best Parameters: {randomized_search.best_params_}')

# Use the best estimator found by RandomizedSearchCV
best_xgb_model = randomized_search.best_estimator_

# Make predictions on the test set using the tuned model
y_test_pred_best_xgb = best_xgb_model.predict(X_test)

# Evaluate the tuned model performance
test_mse_best_xgb = mean_squared_error(y_test, y_test_pred_best_xgb)
test_r2_best_xgb = r2_score(y_test, y_test_pred_best_xgb)

# Print the updated performance metrics
print('Tuned XGBoost Regression Performance:')
print(f'Test MSE: {test_mse_best_xgb}')
print(f'Test R²: {test_r2_best_xgb}')

import matplotlib.pyplot as plt

# Create a scatter plot for actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred_best_xgb, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual AvgTemperature')
plt.ylabel('Predicted AvgTemperature')
plt.title('XGBoost: Actual vs. Predicted AvgTemperature (Tuned Model)')
plt.show()

param_grid = {
    'n_estimators': [300, 500, 700],  # Increase estimators for better learning
    'max_depth': [3, 5, 7],  # Keep the depth moderate to avoid overfitting
    'learning_rate': [0.01, 0.05],  # Lower learning rate for more careful learning
    'subsample': [0.7, 0.8, 0.9],  # Subsampling for more robust learning
    'colsample_bytree': [0.6, 0.8],  # Feature subsampling to reduce overfitting
    'gamma': [0, 0.1, 0.3],  # Control the minimum loss reduction to make splits
    'min_child_weight': [3, 5, 7],  # Larger values to prevent overfitting
    'alpha': [0, 0.1, 0.5],  # L1 regularization (penalizes large weights)
    'lambda': [1, 2, 3],  # L2 regularization (penalizes squared weights)
}

!pip install --upgrade xgboost

import xgboost as xgb
from xgboost import DMatrix

# Convert the training and test datasets into DMatrix (XGBoost's internal data structure)
dtrain = DMatrix(X_train, label=y_train)
dtest = DMatrix(X_test, label=y_test)

# Set parameters for the XGBoost model
params = {
    'objective': 'reg:squarederror',  # Squared error for regression
    'eval_metric': 'rmse',  # Evaluation metric
    'random_state': 42
}

# Train the model using xgboost.train() with early stopping
xgb_model = xgb.train(
    params,
    dtrain,
    num_boost_round=1000,  # Maximum number of boosting rounds
    evals=[(dtest, 'eval')],  # Validation set for early stopping
    early_stopping_rounds=10,  # Stop if no improvement after 10 rounds
    verbose_eval=True  # Print evaluation at each boosting round
)

# Make predictions using the trained model
y_test_pred_xgb = xgb_model.predict(dtest)

# Evaluate the model performance
test_mse_xgb = mean_squared_error(y_test, y_test_pred_xgb)
test_r2_xgb = r2_score(y_test, y_test_pred_xgb)

# Print the performance metrics
print('XGBoost Regression Performance with Early Stopping (using xgboost.train):')
print(f'Test MSE: {test_mse_xgb}')
print(f'Test R²: {test_r2_xgb}')

import matplotlib.pyplot as plt
from xgboost import plot_importance

# Plot feature importance
plot_importance(xgb_model)
plt.show()